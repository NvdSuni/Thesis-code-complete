{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NvdSuni/Thesis-code-complete/blob/main/X_ray_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and loading data"
      ],
      "metadata": {
        "id": "c7kWxAv4VDHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9H2P5AnVFLq",
        "outputId": "68f52d11-bf1e-4e66-e4bf-69325eadf5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print versions\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"PIL (Pillow): {Image.__version__}\")\n",
        "print(f\"tensorflow: {tf.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnmxpssQDfwc",
        "outputId": "2b8b461e-697f-449d-d062-7d056719a680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy: 1.23.5\n",
            "PIL (Pillow): 9.4.0\n",
            "tensorflow: 2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit-keras\n",
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ARQMwXdBN41",
        "outputId": "0f1d8d76-ebe9-450a-a70a-d739c7ae5d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit-keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit-keras) (1.11.3)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit-keras) (0.22.0)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit-keras) (1.23.5)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.10)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths(data_dir):\n",
        "    image_paths = []\n",
        "    for class_dir in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_dir)\n",
        "        for image_file in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, image_file)\n",
        "            image_paths.append(image_path)\n",
        "    return image_paths\n",
        "\n",
        "# Define the paths to the image directories for train, validation, and test\n",
        "train_image_dir = '/content/drive/My Drive/Tilburg University/Thesis/Master Thesis/KneeXrayData/ClsKLData/kneeKL299/train'\n",
        "val_image_dir = '/content/drive/My Drive/Master Thesis/KneeXrayData/ClsKLData/kneeKL299/val'\n",
        "test_image_dir = '/content/drive/My Drive/Master Thesis/KneeXrayData/ClsKLData/kneeKL299/test'\n",
        "\n",
        "# List image file paths in the directories for each set\n",
        "train_image_paths = get_image_paths(train_image_dir)\n",
        "val_image_paths = get_image_paths(val_image_dir)\n",
        "test_image_paths = get_image_paths(test_image_dir)\n",
        "\n",
        "num_classes = 5  # Number of classes\n",
        "\n",
        "# Modify your label creation function to one-hot encode the labels\n",
        "def create_labels(image_paths):\n",
        "    labels = [int(image_path.split(\"/\")[-2]) for image_path in image_paths]\n",
        "    labels = np.array(labels)\n",
        "    labels = to_categorical(labels, num_classes)\n",
        "    return labels\n",
        "\n",
        "train_labels = create_labels(train_image_paths)\n",
        "val_labels = create_labels(val_image_paths)\n",
        "test_labels = create_labels(test_image_paths)\n",
        "# Define the image size\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Manual preprocessing for the training set\n",
        "preprocessed_train_data = []\n",
        "for image_path in train_image_paths:\n",
        "    with Image.open(image_path) as image:\n",
        "        image = image.resize(image_size, Image.LANCZOS)\n",
        "        preprocessed_train_data.append(np.array(image) / 255.0)\n",
        "\n",
        "# Manual preprocessing for the validation set\n",
        "preprocessed_val_data = []\n",
        "for image_path in val_image_paths:\n",
        "    with Image.open(image_path) as image:\n",
        "        image = image.resize(image_size, Image.LANCZOS)\n",
        "        preprocessed_val_data.append(np.array(image) / 255.0)\n",
        "\n",
        "# Manual preprocessing for the test set\n",
        "preprocessed_test_data = []\n",
        "for image_path in test_image_paths:\n",
        "    with Image.open(image_path) as image:\n",
        "        image = image.resize(image_size, Image.LANCZOS)\n",
        "        preprocessed_test_data.append(np.array(image) / 255.0)\n",
        "\n",
        "# Save preprocessed data and labels for each set\n",
        "np.save('/content/drive/My Drive/Master Thesis/KneeXrayData/preprocessed_train_data.npy', np.array(preprocessed_train_data))\n",
        "np.save('/content/drive/My Drive/Master Thesis/KneeXrayData/preprocessed_val_data.npy', np.array(preprocessed_val_data))\n",
        "np.save('/content/drive/My Drive/Master Thesis/KneeXrayData/preprocessed_test_data.npy', np.array(preprocessed_test_data))\n",
        "np.save('/content/drive/My Drive/Master Thesis/KneeXrayData/preprocessed_train_labels.npy', np.array(train_labels))\n",
        "np.save('/content/drive/My Drive/Master Thesis/KneeXrayData/preprocessed_val_labels.npy', np.array(val_labels))\n",
        "np.save('/content/drive/My Drive/Master Thesis/KneeXrayData/preprocessed_test_labels.npy', np.array(test_labels))\n"
      ],
      "metadata": {
        "id": "gBEkYAL60azk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths(data_dir):\n",
        "    image_paths = []\n",
        "    for class_dir in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_dir)\n",
        "        for image_file in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, image_file)\n",
        "            image_paths.append(image_path)\n",
        "    return image_paths\n",
        "\n",
        "# Define the paths to the image directories for train, validation, and test\n",
        "train_image_dir = '/content/drive/My Drive/Tilburg University/Thesis/Master Thesis/KneeXrayData/ClsKLData/kneeKL224/train'\n",
        "val_image_dir = '/content/drive/My Drive/Tilburg University/Thesis/Master Thesis/KneeXrayData/ClsKLData/kneeKL224/val'\n",
        "test_image_dir = '/content/drive/My Drive/Tilburg University/Thesis/Master Thesis/KneeXrayData/ClsKLData/kneeKL224/test'\n",
        "\n",
        "# List image file paths in the directories for each set\n",
        "train_image_paths = get_image_paths(train_image_dir)\n",
        "val_image_paths = get_image_paths(val_image_dir)\n",
        "test_image_paths = get_image_paths(test_image_dir)\n",
        "\n",
        "num_classes = 5  # Number of classes\n",
        "\n",
        "# Modify your label creation function to one-hot encode the labels\n",
        "def create_labels(image_paths):\n",
        "    labels = [int(image_path.split(\"/\")[-2]) for image_path in image_paths]\n",
        "    labels = np.array(labels)\n",
        "    labels = to_categorical(labels, num_classes)\n",
        "    return labels\n",
        "\n",
        "train_labels_224 = create_labels(train_image_paths)\n",
        "val_labels_224 = create_labels(val_image_paths)\n",
        "test_labels_224 = create_labels(test_image_paths)\n",
        "# Define the image size\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Manual preprocessing for the training set\n",
        "preprocessed_train_data_224 = []\n",
        "for image_path in train_image_paths:\n",
        "    with Image.open(image_path) as image:\n",
        "        image = image.resize(image_size, Image.LANCZOS)\n",
        "        preprocessed_train_data_224.append(np.array(image) / 255.0)\n",
        "\n",
        "# Manual preprocessing for the validation set\n",
        "preprocessed_val_data_224 = []\n",
        "for image_path in val_image_paths:\n",
        "    with Image.open(image_path) as image:\n",
        "        image = image.resize(image_size, Image.LANCZOS)\n",
        "        preprocessed_val_data_224.append(np.array(image) / 255.0)\n",
        "\n",
        "# Manual preprocessing for the test set\n",
        "preprocessed_test_data_224 = []\n",
        "for image_path in test_image_paths:\n",
        "    with Image.open(image_path) as image:\n",
        "        image = image.resize(image_size, Image.LANCZOS)\n",
        "        preprocessed_test_data_224.append(np.array(image) / 255.0)\n",
        "\n",
        "# Save preprocessed data and labels for each set\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_data_224.npy', np.array(preprocessed_train_data_224))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_data_224.npy', np.array(preprocessed_val_data_224))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_data_224.npy', np.array(preprocessed_test_data_224))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_labels_224.npy', np.array(train_labels_224))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_labels_224.npy', np.array(val_labels_224))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_labels_224.npy', np.array(test_labels_224))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "xIGqXALZSvuD",
        "outputId": "794772c9-3e21-49f3-82c3-ba414a55ec8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0cb06660c72e>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Save preprocessed data and labels for each set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Master Thesis/KneeXrayData/preprocessed_train_data_224.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Master Thesis/KneeXrayData/preprocessed_val_data_224.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_val_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Master Thesis/KneeXrayData/preprocessed_test_data_224.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessed_train_data = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_data.npy')\n",
        "# preprocessed_val_data = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_data.npy')\n",
        "# preprocessed_test_data = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_data.npy')\n",
        "train_labels = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_labels.npy')\n",
        "val_labels = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_labels.npy')\n",
        "test_labels = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_labels.npy')"
      ],
      "metadata": {
        "id": "cokOD6p9fzja",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "e73d2816-c3f8-4863-a0ed-0309c71e0ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-81950e870821>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# preprocessed_val_data = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_data.npy')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# preprocessed_test_data = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_data.npy')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_labels.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_labels.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_labels.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_labels.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_train_data_224 = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_data_224.npy')\n",
        "preprocessed_val_data_224 = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_data_224.npy')\n",
        "preprocessed_test_data_224 = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_data_224.npy')\n",
        "train_labels_224 = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_labels_224.npy')\n",
        "val_labels_224 = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_labels_224.npy')\n",
        "test_labels_224 = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_labels_224.npy')"
      ],
      "metadata": {
        "id": "nC5TaSvOTiVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Combine preprocessed training data\n",
        "preprocessed_train_data_combined = np.concatenate((preprocessed_train_data, preprocessed_train_data_224), axis=0)\n",
        "preprocessed_val_data_combined = np.concatenate((preprocessed_val_data, preprocessed_val_data_224), axis=0)\n",
        "preprocessed_test_data_combined = np.concatenate((preprocessed_test_data, preprocessed_test_data_224), axis=0)\n",
        "\n",
        "# Combine corresponding training labels\n",
        "train_labels_combined = np.concatenate((train_labels, train_labels_224), axis=0)\n",
        "val_labels_combined = np.concatenate((val_labels, val_labels_224), axis=0)\n",
        "test_labels_combined = np.concatenate((test_labels, test_labels_224), axis=0)\n"
      ],
      "metadata": {
        "id": "t9sZEREEw4r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_data_complete.npy', np.array(preprocessed_train_data_combined))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_data_complete.npy', np.array(preprocessed_val_data_combined))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_data_complete.npy', np.array(preprocessed_test_data_combined))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_labels_complete.npy', np.array(train_labels_combined))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_labels_complete.npy', np.array(val_labels_combined))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_labels_complete.npy', np.array(test_labels_combined))\n"
      ],
      "metadata": {
        "id": "Z8AP5IFvU1Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessed_train_data = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_data_complete.npy')\n",
        "preprocessed_val_data = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_data_complete.npy')\n",
        "# preprocessed_test_data = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_data_complete.npy')\n",
        "# train_labels = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/train_labels_complete_Xray.npy')\n",
        "# val_labels = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/val_labels_complete_Xray.npy')\n",
        "# test_labels = np.load('/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/test_labels_complete_Xray.npy')"
      ],
      "metadata": {
        "id": "lrnEu7y9WWjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels.shape)\n",
        "print(val_labels.shape)\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7onfK-Ftntw",
        "outputId": "2331300a-585d-4e29-8f31-a255ed458e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11569, 5)\n",
            "(1652, 5)\n",
            "(3312, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count occurrences of 1 in each column\n",
        "occurrences = np.sum(train_labels, axis=0)\n",
        "\n",
        "# Print the occurrences\n",
        "for i, count in enumerate(occurrences):\n",
        "    print(f\"Occurrences of 1 in column {i + 1}: {int(count)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdQKIVQzvedX",
        "outputId": "e2614fd0-3925-4688-8dfa-ce9325994834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Occurrences of 1 in column 1: 4575\n",
            "Occurrences of 1 in column 2: 2092\n",
            "Occurrences of 1 in column 3: 3032\n",
            "Occurrences of 1 in column 4: 1524\n",
            "Occurrences of 1 in column 5: 346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardisation"
      ],
      "metadata": {
        "id": "MMM-mdDkVRTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming you already have preprocessed_train_data, preprocessed_val_data, and preprocessed_test_data\n",
        "\n",
        "# Calculate the mean and standard deviation from the training data\n",
        "mean = np.mean(preprocessed_train_data, axis=(0, 1, 2))\n",
        "std = np.std(preprocessed_train_data, axis=(0, 1, 2))\n",
        "\n",
        "# Standardize the data for training, validation, and test sets\n",
        "preprocessed_train_data_standardized = (preprocessed_train_data - mean) / std\n",
        "preprocessed_val_data_standardized = (preprocessed_val_data - mean) / std\n",
        "preprocessed_test_data_standardized = (preprocessed_test_data - mean) / std\n"
      ],
      "metadata": {
        "id": "k-QeRTvjVQM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_train_data_complete.npy', np.array(preprocessed_train_data_standardized))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_val_data_complete.npy', np.array(preprocessed_val_data_standardized))\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/preprocessed_test_data_complete.npy', np.array(preprocessed_test_data_standardized))"
      ],
      "metadata": {
        "id": "p2z0gQgjeufk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_train_data"
      ],
      "metadata": {
        "id": "6GGIe-Ew1bpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full model and Dataset"
      ],
      "metadata": {
        "id": "r435yMqCU8lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5\n",
        "\n",
        "#Model\n",
        "basic_model_xray = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1), name = \"COV1\"),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', name = \"COV2\"),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax', name = \"output_layer\")\n",
        "])\n",
        "\n",
        "basic_model_xray.compile(optimizer='adam',\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "basic_model_xray.summary()\n",
        "#Training\n",
        "history = basic_model_xray.fit(\n",
        "    preprocessed_train_data,\n",
        "    train_labels,\n",
        "    epochs = 20,\n",
        "    validation_data=(preprocessed_val_data, val_labels),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "evaluation_results = basic_model_xray.evaluate(preprocessed_test_data, test_labels)\n",
        "print(\"Evaluation results on the test dataset:\", evaluation_results)"
      ],
      "metadata": {
        "id": "wjWC7QJRWcjK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "outputId": "36ac666d-6785-472f-b792-8dbbdb275e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " COV1 (Conv2D)               (None, 222, 222, 32)      320       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 111, 111, 32)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " COV2 (Conv2D)               (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 760384)            0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               97329280  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 97348741 (371.36 MB)\n",
            "Trainable params: 97348741 (371.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "362/362 [==============================] - 455s 1s/step - loss: 1.8085 - accuracy: 0.3955 - val_loss: 1.3821 - val_accuracy: 0.3923\n",
            "Epoch 2/20\n",
            "166/362 [============>.................] - ETA: 3:32 - loss: 1.2327 - accuracy: 0.4746"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-615f5303a040>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mbasic_model_xray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m history = basic_model_xray.fit(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mpreprocessed_train_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from the \"COV2\" layer (the second convolutional layer) with a name\n",
        "cnn_feature_extractor_xray = Model(inputs=basic_model_xray.input, outputs=basic_model_xray.get_layer(\"COV2\").output, name=\"cnn_feature_extractor_xray2.0\")\n",
        "\n",
        "# Save the CNN feature extractor model in the native Keras format with a name\n",
        "cnn_feature_extractor_xray.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/cnn_feature_extractor_xray2.0')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o6Jd9hlc1Jp",
        "outputId": "156023e3-6bda-49aa-a36c-2c30185d5850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved CNN feature extractor model in the native Keras format\n",
        "cnn_feature_extractor_xray = load_model('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/cnn_feature_extractor_xray2.0')\n"
      ],
      "metadata": {
        "id": "px2cUxLCdJGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c036e92-7ae1-4869-d6c4-7ef20acac63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_train_data1 = preprocessed_train_data[:5785]\n",
        "print(preprocessed_train_data1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu53YrQaRL6I",
        "outputId": "52eb1376-a3c4-4455-8032-01062bdae109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5785, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_train_data2 = preprocessed_train_data[5785:]\n",
        "print(preprocessed_train_data2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf5OZ_o1UFFW",
        "outputId": "5d789ea7-0bd7-4860-e1fa-7d2acff9b38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5784, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the feature extractor to predict features from your data\n",
        "# cnn_features_X_train_Xray = cnn_feature_extractor_xray.predict(preprocessed_train_data)\n",
        "batch_size = 32  # Adjust this batch size according to your available GPU memory\n",
        "\n",
        "cnn_features_X_train_Xray1 = []\n",
        "\n",
        "for i in range(0, len(preprocessed_train_data1), batch_size):\n",
        "    batch_data = preprocessed_train_data1[i:i + batch_size]\n",
        "    features = cnn_feature_extractor_xray.predict(batch_data)\n",
        "    cnn_features_X_train_Xray1.append(features)\n",
        "\n",
        "# Concatenate the results to get the final features\n",
        "cnn_features_X_train_Xray1 = np.concatenate(cnn_features_X_train_Xray1, axis=0)\n",
        "\n",
        "# cnn_features_X_train_Xray = cnn_feature_extractor_xray.predict(preprocessed_val_data)\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/cnn_features_X_train_Xray1.npy', cnn_features_X_train_Xray1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XbrIAfsdLBX",
        "outputId": "3e294901-232d-41c1-88b0-2b6b7482be98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "1/1 [==============================] - 0s 200ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 200ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 207ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 219ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the feature extractor to predict features from your data\n",
        "# cnn_features_X_train_Xray = cnn_feature_extractor_xray.predict(preprocessed_train_data)\n",
        "batch_size = 32  # Adjust this batch size according to your available GPU memory\n",
        "\n",
        "cnn_features_X_train_Xray2 = []\n",
        "\n",
        "for i in range(0, len(preprocessed_train_data2), batch_size):\n",
        "    batch_data = preprocessed_train_data2[i:i + batch_size]\n",
        "    features = cnn_feature_extractor_xray.predict(batch_data)\n",
        "    cnn_features_X_train_Xray2.append(features)\n",
        "\n",
        "# Concatenate the results to get the final features\n",
        "cnn_features_X_train_Xray2 = np.concatenate(cnn_features_X_train_Xray2, axis=0)\n",
        "\n",
        "# cnn_features_X_train_Xray = cnn_feature_extractor_xray.predict(preprocessed_val_data)\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/cnn_features_X_train_Xray2.npy', cnn_features_X_train_Xray2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JM4KMOcUcN3",
        "outputId": "4cd28f19-f820-415d-95bf-6b16c8914747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 384ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 239ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 239ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 237ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_features_X_train_Xray1 = cnn_features_X_train_Xray[:241]\n",
        "cnn_features_X_train_Xray1 = np.concatenate(cnn_features_X_train_Xray1, axis=0)\n",
        "\n",
        "# cnn_features_X_val_Xray = cnn_feature_extractor_xray.predict(preprocessed_val_data)\n",
        "# np.save('/content/drive/My Drive/Tilburg University/Master Thesis/KneeXrayData/cnn_features_X_train_Xray1.npy', cnn_features_X_train_Xray1)"
      ],
      "metadata": {
        "id": "S4TARffGDnE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.sum(cnn_features_X_train_Xray[0] > 0)\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_1Z3tQRr2iv",
        "outputId": "a16b25c2-94c8-46d5-de57-2362ff4bce17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the feature extractor to predict features from your data\n",
        "# cnn_features_X_train_Xray = cnn_feature_extractor_xray.predict(preprocessed_train_data)\n",
        "batch_size = 32  # Adjust this batch size according to your available GPU memory\n",
        "\n",
        "cnn_features_X_val_Xray = []\n",
        "\n",
        "for i in range(0, len(preprocessed_val_data), batch_size):\n",
        "    batch_data = preprocessed_val_data[i:i + batch_size]\n",
        "    features = cnn_feature_extractor_xray.predict(batch_data)\n",
        "    cnn_features_X_val_Xray.append(features)\n",
        "\n",
        "# Concatenate the results to get the final features\n",
        "cnn_features_X_val_Xray = np.concatenate(cnn_features_X_val_Xray, axis=0)\n",
        "\n",
        "# cnn_features_X_val_Xray = cnn_feature_extractor_xray.predict(preprocessed_val_data)\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/cnn_features_X_val_Xray.npy', cnn_features_X_val_Xray)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUNl-l1agL-F",
        "outputId": "845a8802-b9bd-428d-bd8a-7140d08925b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 400ms/step\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 203ms/step\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 209ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnn_features_X_val_Xray.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8DY4-SkzXEg",
        "outputId": "de727e71-f17c-49f9-a817-0432b1c85526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1652, 109, 109, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the feature extractor to predict features from your data\n",
        "# cnn_features_X_train_Xray = cnn_feature_extractor_xray.predict(preprocessed_train_data)\n",
        "batch_size = 32  # Adjust this batch size according to your available GPU memory\n",
        "\n",
        "cnn_features_X_test_Xray = []\n",
        "\n",
        "for i in range(0, len(preprocessed_test_data), batch_size):\n",
        "    batch_data = preprocessed_test_data[i:i + batch_size]\n",
        "    features = cnn_feature_extractor_xray.predict(batch_data)\n",
        "    cnn_features_X_test_Xray.append(features)\n",
        "\n",
        "# Concatenate the results to get the final features\n",
        "cnn_features_X_test_Xray = np.concatenate(cnn_features_X_test_Xray, axis=0)\n",
        "\n",
        "# cnn_features_X_val_Xray = cnn_feature_extractor_xray.predict(preprocessed_val_data)\n",
        "np.save('/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/cnn_features_X_test_Xray.npy', cnn_features_X_test_Xray)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NC01td6ggmU",
        "outputId": "8f178525-c19a-478f-fa76-a836b038278c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 221ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "1/1 [==============================] - 0s 206ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 228ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 203ms/step\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 202ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "1/1 [==============================] - 0s 235ms/step\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnn_features_X_test_Xray.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cxT0k51zyc4",
        "outputId": "89e77089-ffa4-4dcf-b0c2-fba5149865b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1656, 109, 109, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transfer Learning"
      ],
      "metadata": {
        "id": "RHNjp9s50CAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'y_train' and 'y_val' are your one-hot encoded label arrays\n",
        "y_train = np.argmax(train_labels, axis=1)\n",
        "y_val = np.argmax(val_labels, axis=1)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "seed = 42  # You can use any integer as the seed\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Shuffle the training data\n",
        "shuffled_indices = np.arange(len(preprocessed_train_data))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "X_train_shuffled = preprocessed_train_data[shuffled_indices]\n",
        "y_train_shuffled = y_train[shuffled_indices]\n",
        "\n",
        "# Shuffle the validation data\n",
        "shuffled_indices = np.arange(len(preprocessed_val_data))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "X_val_shuffled = preprocessed_val_data[shuffled_indices]\n",
        "y_val_shuffled = y_val[shuffled_indices]\n",
        "\n"
      ],
      "metadata": {
        "id": "l52JmX77MtVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50  # You can choose a different pre-trained model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "# Assuming you have grayscale images loaded in preprocessed_train_data\n",
        "X_train_complete = np.stack((X_train_shuffled,) * 3, axis=-1)  # Duplicating the single channel into 3 channels\n",
        "X_val_complete = np.stack((X_val_shuffled,) * 3, axis=-1)  # Duplicating the single channel into 3 channels\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dDXdFySXtL9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1762d8f5-3863-45f8-86c4-84a5ffaad6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)   (None, 230, 230, 3)          0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)         (None, 112, 112, 64)         9472      ['conv1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalizati  (None, 112, 112, 64)         256       ['conv1_conv[0][0]']          \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)     (None, 112, 112, 64)         0         ['conv1_bn[0][0]']            \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)   (None, 114, 114, 64)         0         ['conv1_relu[0][0]']          \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)   (None, 56, 56, 64)           0         ['pool1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2  (None, 56, 56, 64)           4160      ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2  (None, 56, 56, 256)          16640     ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)      (None, 56, 56, 256)          0         ['conv2_block1_0_bn[0][0]',   \n",
            "                                                                     'conv2_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activati  (None, 56, 56, 256)          0         ['conv2_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2  (None, 56, 56, 64)           16448     ['conv2_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)      (None, 56, 56, 256)          0         ['conv2_block1_out[0][0]',    \n",
            "                                                                     'conv2_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activati  (None, 56, 56, 256)          0         ['conv2_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2  (None, 56, 56, 64)           16448     ['conv2_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)      (None, 56, 56, 256)          0         ['conv2_block2_out[0][0]',    \n",
            "                                                                     'conv2_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activati  (None, 56, 56, 256)          0         ['conv2_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2  (None, 28, 28, 128)          32896     ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2  (None, 28, 28, 512)          131584    ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)      (None, 28, 28, 512)          0         ['conv3_block1_0_bn[0][0]',   \n",
            "                                                                     'conv3_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activati  (None, 28, 28, 512)          0         ['conv3_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)      (None, 28, 28, 512)          0         ['conv3_block1_out[0][0]',    \n",
            "                                                                     'conv3_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activati  (None, 28, 28, 512)          0         ['conv3_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)      (None, 28, 28, 512)          0         ['conv3_block2_out[0][0]',    \n",
            "                                                                     'conv3_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activati  (None, 28, 28, 512)          0         ['conv3_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)      (None, 28, 28, 512)          0         ['conv3_block3_out[0][0]',    \n",
            "                                                                     'conv3_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activati  (None, 28, 28, 512)          0         ['conv3_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2  (None, 14, 14, 256)          131328    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2  (None, 14, 14, 1024)         525312    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_0_bn[0][0]',   \n",
            "                                                                     'conv4_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_out[0][0]',    \n",
            "                                                                     'conv4_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block2_out[0][0]',    \n",
            "                                                                     'conv4_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block3_out[0][0]',    \n",
            "                                                                     'conv4_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block5_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block5_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block5_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block4_out[0][0]',    \n",
            "                                                                     'conv4_block5_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block5_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block5_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block6_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block6_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block6_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block5_out[0][0]',    \n",
            "                                                                     'conv4_block6_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block6_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2  (None, 7, 7, 512)            524800    ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2  (None, 7, 7, 2048)           2099200   ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_0_bn[0][0]',   \n",
            "                                                                     'conv5_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2  (None, 7, 7, 512)            1049088   ['conv5_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_out[0][0]',    \n",
            "                                                                     'conv5_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2  (None, 7, 7, 512)            1049088   ['conv5_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block2_out[0][0]',    \n",
            "                                                                     'conv5_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23587712 (89.98 MB)\n",
            "Trainable params: 23534592 (89.78 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "feature_extractor = Model(inputs=base_model.input, outputs=x)\n"
      ],
      "metadata": {
        "id": "HFModn34taJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_train = feature_extractor.predict(X_train_complete)\n",
        "features_val = feature_extractor.predict(X_val_complete)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbzLcgZ2tcVv",
        "outputId": "32c1e728-f8c5-455d-92a8-72600f7bda6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "181/181 [==============================] - 25s 89ms/step\n",
            "26/26 [==============================] - 3s 125ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train_shuffled[:500])\n"
      ],
      "metadata": {
        "id": "XNKbc2lGoZ7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d55365-ded1-476c-d89b-19b569d922d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 2 1 0 3 1 0 4 0 2 3 0 1 2 0 2 0 0 0 2 2 0 0 1 2 0 2 2 3 0 0 3 0 0 2 2\n",
            " 2 2 0 2 2 1 4 2 1 3 0 0 0 1 1 3 2 0 1 2 1 3 1 0 0 1 3 1 1 0 2 0 2 1 3 2 0\n",
            " 0 3 2 2 1 3 0 1 4 3 4 2 2 2 0 0 2 0 1 3 0 0 1 0 2 3 3 0 0 2 0 0 2 4 0 1 1\n",
            " 0 0 0 2 0 1 0 0 0 2 3 0 1 1 3 2 0 0 2 0 1 2 2 3 0 2 4 2 0 1 0 0 2 2 2 2 2\n",
            " 2 3 2 0 3 0 3 0 2 1 1 0 1 0 0 0 1 0 1 1 2 0 0 0 1 1 1 0 2 3 0 2 0 0 2 2 3\n",
            " 0 2 0 2 1 3 0 1 3 0 0 1 2 3 1 1 2 0 0 0 2 3 0 2 0 0 0 1 0 0 3 0 0 0 2 2 0\n",
            " 2 0 4 3 2 0 0 0 1 3 2 0 3 3 2 1 0 1 3 2 0 0 0 2 2 2 0 0 0 0 0 3 2 2 2 4 2\n",
            " 3 1 2 3 2 0 0 0 2 2 0 1 0 2 3 0 0 0 1 2 0 0 0 0 1 3 0 1 0 2 1 1 0 4 2 0 0\n",
            " 2 0 3 2 2 0 2 0 3 2 1 1 2 2 0 0 1 2 2 1 0 0 0 2 0 2 2 0 2 3 0 2 2 2 0 1 2\n",
            " 3 2 1 0 2 1 1 2 1 2 1 1 0 3 2 1 1 3 0 1 0 0 0 0 3 0 3 0 2 0 0 3 0 4 0 0 0\n",
            " 0 0 0 0 0 1 2 0 0 3 0 0 3 2 2 0 0 1 0 0 0 2 0 0 1 2 2 0 0 0 2 0 2 0 3 2 1\n",
            " 0 1 4 0 3 3 0 1 0 3 0 0 0 2 2 0 2 0 2 2 2 2 2 0 2 3 0 2 1 0 0 1 0 0 0 0 3\n",
            " 0 0 0 4 0 0 2 0 3 3 0 0 2 1 3 2 0 1 3 0 1 2 0 0 1 0 0 3 3 0 0 0 3 3 0 0 0\n",
            " 2 0 0 0 0 0 1 2 0 2 2 0 0 3 3 2 2 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm_classifier = svm.SVC(kernel='linear')  # You can choose different kernels (e.g., linear, rbf) based on your problem.\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 128  # You can adjust this based on your available memory and dataset size\n",
        "\n",
        "# Initialize variables to monitor accuracy\n",
        "num_samples = len(features_train)\n",
        "accuracy_history = []  # To store accuracy at each batch\n",
        "\n",
        "# Train the SVM model with batching and monitoring progress\n",
        "for i in tqdm(range(0, num_samples, batch_size)):\n",
        "    batch_X = features_train[i:i + batch_size]\n",
        "    batch_y = y_train_shuffled[i:i + batch_size]\n",
        "    svm_classifier.fit(batch_X, batch_y)\n",
        "\n",
        "    # Calculate accuracy on a validation set at each batch\n",
        "    y_pred_val = svm_classifier.predict(features_val)\n",
        "    accuracy = accuracy_score(y_val_shuffled, y_pred_val)\n",
        "    accuracy_history.append(accuracy)\n",
        "\n",
        "# Print accuracy history\n",
        "print(\"Accuracy History:\")\n",
        "print(accuracy_history)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_classifier.predict(features_val)\n",
        "\n",
        "# Evaluate the SVM model\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy = accuracy_score(y_val_shuffled, y_pred)\n",
        "print(\"Final Accuracy:\", accuracy)\n",
        "\n",
        "# You can also print a more detailed report\n",
        "report = classification_report(y_val_shuffled, y_pred)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XeP3DL5md7j",
        "outputId": "166bc97d-5f21-41c4-f3f7-79443556af8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 46/46 [00:04<00:00,  9.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy History:\n",
            "[0.40314769975786924, 0.3934624697336562, 0.3559322033898305, 0.3789346246973366, 0.400726392251816, 0.3644067796610169, 0.3910411622276029, 0.37772397094430993, 0.3135593220338983, 0.4092009685230024, 0.37530266343825663, 0.39588377723970947, 0.35472154963680386, 0.4019370460048426, 0.3946731234866828, 0.3934624697336562, 0.35472154963680386, 0.3668280871670702, 0.3813559322033898, 0.387409200968523, 0.39951573849878935, 0.3474576271186441, 0.3728813559322034, 0.400726392251816, 0.37530266343825663, 0.3728813559322034, 0.38619854721549635, 0.3983050847457627, 0.3268765133171913, 0.41041162227602906, 0.3280871670702179, 0.37409200968523004, 0.4067796610169492, 0.4043583535108959, 0.3922518159806295, 0.3704600484261501, 0.35351089588377727, 0.40556900726392253, 0.3922518159806295, 0.3813559322033898, 0.42736077481840196, 0.36561743341404357, 0.400726392251816, 0.36561743341404357, 0.39709443099273606, 0.2457627118644068]\n",
            "Final Accuracy: 0.2457627118644068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.12      0.19       328\n",
            "           1       0.18      0.07      0.10       153\n",
            "           2       0.27      0.58      0.37       212\n",
            "           3       0.14      0.28      0.19       106\n",
            "           4       0.08      0.04      0.05        27\n",
            "\n",
            "    accuracy                           0.25       826\n",
            "   macro avg       0.22      0.22      0.18       826\n",
            "weighted avg       0.29      0.25      0.21       826\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust hyperparameters\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_classifier.fit(features_train, y_train_shuffled)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf = rf_classifier.predict(features_val)\n",
        "\n",
        "# Evaluate the Random Forest model\n",
        "accuracy_rf = accuracy_score(y_val_shuffled, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
        "\n",
        "# You can also print a more detailed report\n",
        "report_rf = classification_report(y_val_shuffled, y_pred_rf)\n",
        "print(report_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OZ6g516PVra",
        "outputId": "06c58c22-f18f-47c9-85af-d2d34e8f2a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.42493946731234866\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.83      0.58       328\n",
            "           1       0.18      0.01      0.02       153\n",
            "           2       0.36      0.32      0.34       212\n",
            "           3       0.40      0.08      0.13       106\n",
            "           4       1.00      0.07      0.14        27\n",
            "\n",
            "    accuracy                           0.42       826\n",
            "   macro avg       0.48      0.26      0.24       826\n",
            "weighted avg       0.39      0.42      0.34       826\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create an XGBoost classifier\n",
        "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=5, random_state=42)  # You can adjust hyperparameters\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_classifier.fit(features_train, y_train_shuffled)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_xgb = xgb_classifier.predict(features_val)\n",
        "\n",
        "# Evaluate the XGBoost model\n",
        "accuracy_xgb = accuracy_score(y_val_shuffled, y_pred_xgb)\n",
        "print(\"XGBoost Accuracy:\", accuracy_xgb)\n",
        "\n",
        "# You can also print a more detailed report\n",
        "report_xgb = classification_report(y_val_shuffled, y_pred_xgb)\n",
        "print(report_xgb)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnMwoj9iPl3Z",
        "outputId": "e4751ccb-533a-4cd9-a4bd-98682f177308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Accuracy: 0.4515738498789346\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.77      0.61       328\n",
            "           1       0.20      0.06      0.09       153\n",
            "           2       0.37      0.41      0.39       212\n",
            "           3       0.50      0.23      0.31       106\n",
            "           4       1.00      0.11      0.20        27\n",
            "\n",
            "    accuracy                           0.45       826\n",
            "   macro avg       0.52      0.31      0.32       826\n",
            "weighted avg       0.43      0.45      0.40       826\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Assuming features_train and features_val have shape (None, num_features)\n",
        "# Where num_features is the output dimension of the ResNet50 feature extraction\n",
        "# e.g., num_features = 2048 if you used a specific layer for feature extraction\n",
        "num_features = 2048\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming y_train and y_val are your original labels (e.g., [0, 1, 2, 3, 4])\n",
        "# One-hot encode the labels\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train_shuffled, num_classes=5)\n",
        "y_val_one_hot = tf.keras.utils.to_categorical(y_val_shuffled, num_classes=5)\n",
        "\n",
        "\n",
        "\n",
        "# Define a custom learning rate\n",
        "custom_learning_rate = 0.1  # You can adjust this value as needed\n",
        "\n",
        "# Create an Adam optimizer with the custom learning rate\n",
        "custom_optimizer = keras.optimizers.Adam(learning_rate=custom_learning_rate)\n",
        "\n",
        "# Create an FNN model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(num_features,)),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "num_epochs = 250\n",
        "\n",
        "history = model.fit(features_train, y_train_one_hot, batch_size=batch_size, epochs=num_epochs, validation_data=(features_val, y_val_one_hot))\n",
        "\n",
        "# Evaluate the FNN model\n",
        "test_loss, test_accuracy = model.evaluate(features_val, y_val_one_hot, verbose=2)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n",
        "# Assuming you have trained and made predictions with your FNN model\n",
        "y_pred = model.predict(features_val)  # Adjust based on your model and features\n",
        "\n",
        "# Convert one-hot encoded labels to their original format\n",
        "y_val_original = np.argmax(y_val_one_hot, axis=1)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_val_original, y_pred_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUNe0F1kQAzC",
        "outputId": "e1377346-ddca-4abe-e98c-5e1804370e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "46/46 [==============================] - 2s 8ms/step - loss: 1.4233 - accuracy: 0.3845 - val_loss: 1.3963 - val_accuracy: 0.3983\n",
            "Epoch 2/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3945 - accuracy: 0.3965 - val_loss: 1.4002 - val_accuracy: 0.3414\n",
            "Epoch 3/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3878 - accuracy: 0.3887 - val_loss: 1.4136 - val_accuracy: 0.2567\n",
            "Epoch 4/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3878 - accuracy: 0.3946 - val_loss: 1.3895 - val_accuracy: 0.4140\n",
            "Epoch 5/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3684 - accuracy: 0.4053 - val_loss: 1.3677 - val_accuracy: 0.4189\n",
            "Epoch 6/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3646 - accuracy: 0.4075 - val_loss: 1.3651 - val_accuracy: 0.4213\n",
            "Epoch 7/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3506 - accuracy: 0.4058 - val_loss: 1.3436 - val_accuracy: 0.4019\n",
            "Epoch 8/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3390 - accuracy: 0.4039 - val_loss: 1.3599 - val_accuracy: 0.4031\n",
            "Epoch 9/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3216 - accuracy: 0.4261 - val_loss: 1.3218 - val_accuracy: 0.4274\n",
            "Epoch 10/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3227 - accuracy: 0.4202 - val_loss: 1.3748 - val_accuracy: 0.3983\n",
            "Epoch 11/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3192 - accuracy: 0.4186 - val_loss: 1.3082 - val_accuracy: 0.4177\n",
            "Epoch 12/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2961 - accuracy: 0.4340 - val_loss: 1.3722 - val_accuracy: 0.3874\n",
            "Epoch 13/250\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.2985 - accuracy: 0.4345 - val_loss: 1.3020 - val_accuracy: 0.4116\n",
            "Epoch 14/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2905 - accuracy: 0.4357 - val_loss: 1.2887 - val_accuracy: 0.4274\n",
            "Epoch 15/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2738 - accuracy: 0.4425 - val_loss: 1.2732 - val_accuracy: 0.4431\n",
            "Epoch 16/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2543 - accuracy: 0.4596 - val_loss: 1.2788 - val_accuracy: 0.4310\n",
            "Epoch 17/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2509 - accuracy: 0.4494 - val_loss: 1.2844 - val_accuracy: 0.4370\n",
            "Epoch 18/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2380 - accuracy: 0.4574 - val_loss: 1.3201 - val_accuracy: 0.4165\n",
            "Epoch 19/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2454 - accuracy: 0.4549 - val_loss: 1.2476 - val_accuracy: 0.4467\n",
            "Epoch 20/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2309 - accuracy: 0.4641 - val_loss: 1.5944 - val_accuracy: 0.2869\n",
            "Epoch 21/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.3410 - accuracy: 0.4157 - val_loss: 1.3494 - val_accuracy: 0.3910\n",
            "Epoch 22/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2490 - accuracy: 0.4544 - val_loss: 1.3338 - val_accuracy: 0.4140\n",
            "Epoch 23/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2558 - accuracy: 0.4501 - val_loss: 1.2800 - val_accuracy: 0.4370\n",
            "Epoch 24/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2165 - accuracy: 0.4674 - val_loss: 1.2443 - val_accuracy: 0.4540\n",
            "Epoch 25/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2173 - accuracy: 0.4750 - val_loss: 1.2424 - val_accuracy: 0.4709\n",
            "Epoch 26/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2045 - accuracy: 0.4812 - val_loss: 1.2375 - val_accuracy: 0.4770\n",
            "Epoch 27/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2089 - accuracy: 0.4690 - val_loss: 1.2838 - val_accuracy: 0.4298\n",
            "Epoch 28/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2278 - accuracy: 0.4622 - val_loss: 1.2441 - val_accuracy: 0.4685\n",
            "Epoch 29/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2089 - accuracy: 0.4748 - val_loss: 1.2229 - val_accuracy: 0.4855\n",
            "Epoch 30/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1955 - accuracy: 0.4821 - val_loss: 1.2278 - val_accuracy: 0.4709\n",
            "Epoch 31/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1887 - accuracy: 0.4838 - val_loss: 1.2630 - val_accuracy: 0.4613\n",
            "Epoch 32/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1936 - accuracy: 0.4840 - val_loss: 1.2366 - val_accuracy: 0.4709\n",
            "Epoch 33/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1989 - accuracy: 0.4748 - val_loss: 1.2421 - val_accuracy: 0.4927\n",
            "Epoch 34/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1824 - accuracy: 0.4888 - val_loss: 1.2552 - val_accuracy: 0.4552\n",
            "Epoch 35/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1761 - accuracy: 0.4907 - val_loss: 1.2897 - val_accuracy: 0.4516\n",
            "Epoch 36/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1984 - accuracy: 0.4757 - val_loss: 1.2594 - val_accuracy: 0.4431\n",
            "Epoch 37/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1762 - accuracy: 0.4940 - val_loss: 1.2498 - val_accuracy: 0.4322\n",
            "Epoch 38/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1900 - accuracy: 0.4818 - val_loss: 1.2118 - val_accuracy: 0.4915\n",
            "Epoch 39/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1651 - accuracy: 0.5006 - val_loss: 1.2429 - val_accuracy: 0.4770\n",
            "Epoch 40/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1674 - accuracy: 0.4994 - val_loss: 1.2172 - val_accuracy: 0.4746\n",
            "Epoch 41/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1807 - accuracy: 0.4965 - val_loss: 1.2189 - val_accuracy: 0.4770\n",
            "Epoch 42/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1780 - accuracy: 0.4830 - val_loss: 1.3509 - val_accuracy: 0.4213\n",
            "Epoch 43/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1652 - accuracy: 0.4975 - val_loss: 1.2073 - val_accuracy: 0.4915\n",
            "Epoch 44/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1575 - accuracy: 0.5070 - val_loss: 1.2706 - val_accuracy: 0.4189\n",
            "Epoch 45/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1719 - accuracy: 0.4902 - val_loss: 1.2341 - val_accuracy: 0.4806\n",
            "Epoch 46/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1417 - accuracy: 0.5136 - val_loss: 1.3038 - val_accuracy: 0.4613\n",
            "Epoch 47/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1581 - accuracy: 0.4984 - val_loss: 1.2706 - val_accuracy: 0.4588\n",
            "Epoch 48/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1791 - accuracy: 0.4890 - val_loss: 1.2693 - val_accuracy: 0.4504\n",
            "Epoch 49/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1533 - accuracy: 0.5054 - val_loss: 1.2465 - val_accuracy: 0.4613\n",
            "Epoch 50/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1398 - accuracy: 0.5136 - val_loss: 1.2221 - val_accuracy: 0.4927\n",
            "Epoch 51/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1647 - accuracy: 0.5006 - val_loss: 1.2119 - val_accuracy: 0.4939\n",
            "Epoch 52/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1656 - accuracy: 0.5018 - val_loss: 1.2047 - val_accuracy: 0.4939\n",
            "Epoch 53/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1413 - accuracy: 0.5080 - val_loss: 1.2137 - val_accuracy: 0.4879\n",
            "Epoch 54/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1309 - accuracy: 0.5134 - val_loss: 1.2374 - val_accuracy: 0.4709\n",
            "Epoch 55/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1288 - accuracy: 0.5165 - val_loss: 1.3562 - val_accuracy: 0.4334\n",
            "Epoch 56/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1428 - accuracy: 0.5146 - val_loss: 1.2447 - val_accuracy: 0.4552\n",
            "Epoch 57/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1270 - accuracy: 0.5129 - val_loss: 1.2200 - val_accuracy: 0.4758\n",
            "Epoch 58/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1255 - accuracy: 0.5172 - val_loss: 1.2977 - val_accuracy: 0.4455\n",
            "Epoch 59/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1265 - accuracy: 0.5165 - val_loss: 1.2067 - val_accuracy: 0.4915\n",
            "Epoch 60/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1300 - accuracy: 0.5118 - val_loss: 1.2424 - val_accuracy: 0.4806\n",
            "Epoch 61/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1471 - accuracy: 0.5098 - val_loss: 1.2135 - val_accuracy: 0.4903\n",
            "Epoch 62/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1401 - accuracy: 0.5129 - val_loss: 1.2105 - val_accuracy: 0.4927\n",
            "Epoch 63/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1578 - accuracy: 0.5001 - val_loss: 1.2767 - val_accuracy: 0.4576\n",
            "Epoch 64/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1140 - accuracy: 0.5214 - val_loss: 1.1980 - val_accuracy: 0.5000\n",
            "Epoch 65/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1078 - accuracy: 0.5219 - val_loss: 1.2214 - val_accuracy: 0.4818\n",
            "Epoch 66/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1174 - accuracy: 0.5231 - val_loss: 1.2109 - val_accuracy: 0.4770\n",
            "Epoch 67/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1003 - accuracy: 0.5297 - val_loss: 1.2454 - val_accuracy: 0.4722\n",
            "Epoch 68/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1030 - accuracy: 0.5257 - val_loss: 1.2450 - val_accuracy: 0.4831\n",
            "Epoch 69/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1141 - accuracy: 0.5234 - val_loss: 1.2066 - val_accuracy: 0.5012\n",
            "Epoch 70/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1212 - accuracy: 0.5120 - val_loss: 1.2534 - val_accuracy: 0.4709\n",
            "Epoch 71/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1214 - accuracy: 0.5165 - val_loss: 1.3168 - val_accuracy: 0.4492\n",
            "Epoch 72/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1307 - accuracy: 0.5105 - val_loss: 1.2045 - val_accuracy: 0.4976\n",
            "Epoch 73/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0887 - accuracy: 0.5383 - val_loss: 1.2128 - val_accuracy: 0.4843\n",
            "Epoch 74/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1107 - accuracy: 0.5227 - val_loss: 1.2397 - val_accuracy: 0.4697\n",
            "Epoch 75/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1009 - accuracy: 0.5340 - val_loss: 1.2630 - val_accuracy: 0.4673\n",
            "Epoch 76/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1119 - accuracy: 0.5222 - val_loss: 1.1934 - val_accuracy: 0.5133\n",
            "Epoch 77/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1019 - accuracy: 0.5266 - val_loss: 1.4033 - val_accuracy: 0.4177\n",
            "Epoch 78/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1278 - accuracy: 0.5139 - val_loss: 1.2060 - val_accuracy: 0.4964\n",
            "Epoch 79/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1069 - accuracy: 0.5241 - val_loss: 1.1903 - val_accuracy: 0.5121\n",
            "Epoch 80/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0890 - accuracy: 0.5328 - val_loss: 1.2119 - val_accuracy: 0.4891\n",
            "Epoch 81/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0803 - accuracy: 0.5376 - val_loss: 1.2014 - val_accuracy: 0.5061\n",
            "Epoch 82/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0713 - accuracy: 0.5470 - val_loss: 1.2475 - val_accuracy: 0.4770\n",
            "Epoch 83/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0821 - accuracy: 0.5409 - val_loss: 1.2187 - val_accuracy: 0.4709\n",
            "Epoch 84/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0832 - accuracy: 0.5361 - val_loss: 1.2585 - val_accuracy: 0.4407\n",
            "Epoch 85/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0915 - accuracy: 0.5283 - val_loss: 1.2607 - val_accuracy: 0.4709\n",
            "Epoch 86/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1055 - accuracy: 0.5214 - val_loss: 1.2263 - val_accuracy: 0.4879\n",
            "Epoch 87/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0844 - accuracy: 0.5381 - val_loss: 1.2305 - val_accuracy: 0.4758\n",
            "Epoch 88/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0745 - accuracy: 0.5439 - val_loss: 1.2125 - val_accuracy: 0.4927\n",
            "Epoch 89/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0580 - accuracy: 0.5471 - val_loss: 1.2147 - val_accuracy: 0.5000\n",
            "Epoch 90/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0704 - accuracy: 0.5487 - val_loss: 1.2842 - val_accuracy: 0.4370\n",
            "Epoch 91/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0793 - accuracy: 0.5369 - val_loss: 1.2036 - val_accuracy: 0.5121\n",
            "Epoch 92/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.1045 - accuracy: 0.5298 - val_loss: 1.2225 - val_accuracy: 0.4903\n",
            "Epoch 93/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0617 - accuracy: 0.5513 - val_loss: 1.2386 - val_accuracy: 0.4855\n",
            "Epoch 94/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0833 - accuracy: 0.5385 - val_loss: 1.2200 - val_accuracy: 0.4915\n",
            "Epoch 95/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0821 - accuracy: 0.5385 - val_loss: 1.2651 - val_accuracy: 0.4613\n",
            "Epoch 96/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0585 - accuracy: 0.5489 - val_loss: 1.2044 - val_accuracy: 0.5109\n",
            "Epoch 97/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0463 - accuracy: 0.5575 - val_loss: 1.2603 - val_accuracy: 0.4552\n",
            "Epoch 98/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0785 - accuracy: 0.5433 - val_loss: 1.2479 - val_accuracy: 0.4661\n",
            "Epoch 99/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0773 - accuracy: 0.5387 - val_loss: 1.2075 - val_accuracy: 0.5169\n",
            "Epoch 100/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0668 - accuracy: 0.5492 - val_loss: 1.2284 - val_accuracy: 0.4891\n",
            "Epoch 101/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0422 - accuracy: 0.5535 - val_loss: 1.3155 - val_accuracy: 0.4431\n",
            "Epoch 102/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0735 - accuracy: 0.5428 - val_loss: 1.2741 - val_accuracy: 0.4770\n",
            "Epoch 103/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0501 - accuracy: 0.5567 - val_loss: 1.2281 - val_accuracy: 0.5024\n",
            "Epoch 104/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0531 - accuracy: 0.5516 - val_loss: 1.2471 - val_accuracy: 0.4879\n",
            "Epoch 105/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0562 - accuracy: 0.5459 - val_loss: 1.2217 - val_accuracy: 0.5048\n",
            "Epoch 106/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0475 - accuracy: 0.5537 - val_loss: 1.4333 - val_accuracy: 0.4092\n",
            "Epoch 107/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0395 - accuracy: 0.5586 - val_loss: 1.2259 - val_accuracy: 0.4927\n",
            "Epoch 108/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0320 - accuracy: 0.5658 - val_loss: 1.3691 - val_accuracy: 0.4564\n",
            "Epoch 109/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0993 - accuracy: 0.5293 - val_loss: 1.3324 - val_accuracy: 0.4600\n",
            "Epoch 110/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0772 - accuracy: 0.5347 - val_loss: 1.2166 - val_accuracy: 0.5061\n",
            "Epoch 111/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0666 - accuracy: 0.5437 - val_loss: 1.3346 - val_accuracy: 0.3898\n",
            "Epoch 112/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0602 - accuracy: 0.5480 - val_loss: 1.2229 - val_accuracy: 0.5073\n",
            "Epoch 113/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0541 - accuracy: 0.5584 - val_loss: 1.2104 - val_accuracy: 0.5085\n",
            "Epoch 114/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0423 - accuracy: 0.5591 - val_loss: 1.2694 - val_accuracy: 0.4673\n",
            "Epoch 115/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0273 - accuracy: 0.5591 - val_loss: 1.3154 - val_accuracy: 0.4576\n",
            "Epoch 116/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0524 - accuracy: 0.5553 - val_loss: 1.2877 - val_accuracy: 0.4746\n",
            "Epoch 117/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0264 - accuracy: 0.5608 - val_loss: 1.2312 - val_accuracy: 0.4952\n",
            "Epoch 118/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0257 - accuracy: 0.5700 - val_loss: 1.2581 - val_accuracy: 0.4794\n",
            "Epoch 119/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0261 - accuracy: 0.5681 - val_loss: 1.2800 - val_accuracy: 0.4794\n",
            "Epoch 120/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0164 - accuracy: 0.5679 - val_loss: 1.2659 - val_accuracy: 0.4891\n",
            "Epoch 121/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0126 - accuracy: 0.5700 - val_loss: 1.2290 - val_accuracy: 0.5048\n",
            "Epoch 122/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0097 - accuracy: 0.5677 - val_loss: 1.3354 - val_accuracy: 0.4370\n",
            "Epoch 123/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0459 - accuracy: 0.5591 - val_loss: 1.2675 - val_accuracy: 0.4843\n",
            "Epoch 124/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0087 - accuracy: 0.5769 - val_loss: 1.3404 - val_accuracy: 0.4334\n",
            "Epoch 125/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0550 - accuracy: 0.5490 - val_loss: 1.2692 - val_accuracy: 0.4782\n",
            "Epoch 126/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0449 - accuracy: 0.5554 - val_loss: 1.2903 - val_accuracy: 0.4407\n",
            "Epoch 127/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0121 - accuracy: 0.5726 - val_loss: 1.2792 - val_accuracy: 0.4915\n",
            "Epoch 128/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0230 - accuracy: 0.5663 - val_loss: 1.2582 - val_accuracy: 0.4879\n",
            "Epoch 129/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0030 - accuracy: 0.5743 - val_loss: 1.2992 - val_accuracy: 0.4831\n",
            "Epoch 130/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0168 - accuracy: 0.5708 - val_loss: 1.3524 - val_accuracy: 0.4637\n",
            "Epoch 131/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0003 - accuracy: 0.5746 - val_loss: 1.2734 - val_accuracy: 0.4988\n",
            "Epoch 132/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0406 - accuracy: 0.5570 - val_loss: 1.2833 - val_accuracy: 0.4782\n",
            "Epoch 133/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0164 - accuracy: 0.5720 - val_loss: 1.2329 - val_accuracy: 0.5085\n",
            "Epoch 134/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9982 - accuracy: 0.5805 - val_loss: 1.2690 - val_accuracy: 0.5085\n",
            "Epoch 135/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0138 - accuracy: 0.5707 - val_loss: 1.3206 - val_accuracy: 0.4661\n",
            "Epoch 136/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0531 - accuracy: 0.5385 - val_loss: 1.3548 - val_accuracy: 0.4576\n",
            "Epoch 137/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0397 - accuracy: 0.5573 - val_loss: 1.2648 - val_accuracy: 0.5061\n",
            "Epoch 138/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9957 - accuracy: 0.5805 - val_loss: 1.2690 - val_accuracy: 0.5109\n",
            "Epoch 139/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0198 - accuracy: 0.5677 - val_loss: 1.2546 - val_accuracy: 0.4976\n",
            "Epoch 140/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9875 - accuracy: 0.5848 - val_loss: 1.2620 - val_accuracy: 0.5073\n",
            "Epoch 141/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9967 - accuracy: 0.5783 - val_loss: 1.2637 - val_accuracy: 0.5012\n",
            "Epoch 142/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9945 - accuracy: 0.5803 - val_loss: 1.3467 - val_accuracy: 0.4722\n",
            "Epoch 143/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9970 - accuracy: 0.5779 - val_loss: 1.2725 - val_accuracy: 0.5133\n",
            "Epoch 144/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0021 - accuracy: 0.5790 - val_loss: 1.3169 - val_accuracy: 0.4697\n",
            "Epoch 145/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0115 - accuracy: 0.5686 - val_loss: 1.2661 - val_accuracy: 0.5012\n",
            "Epoch 146/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9990 - accuracy: 0.5805 - val_loss: 1.3354 - val_accuracy: 0.4831\n",
            "Epoch 147/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9896 - accuracy: 0.5809 - val_loss: 1.2700 - val_accuracy: 0.4891\n",
            "Epoch 148/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9988 - accuracy: 0.5739 - val_loss: 1.2576 - val_accuracy: 0.5085\n",
            "Epoch 149/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9713 - accuracy: 0.5881 - val_loss: 1.3170 - val_accuracy: 0.4915\n",
            "Epoch 150/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9779 - accuracy: 0.5862 - val_loss: 1.2906 - val_accuracy: 0.4831\n",
            "Epoch 151/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9847 - accuracy: 0.5836 - val_loss: 1.3510 - val_accuracy: 0.4758\n",
            "Epoch 152/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9877 - accuracy: 0.5762 - val_loss: 1.3945 - val_accuracy: 0.4734\n",
            "Epoch 153/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9957 - accuracy: 0.5762 - val_loss: 1.2790 - val_accuracy: 0.5085\n",
            "Epoch 154/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9748 - accuracy: 0.5826 - val_loss: 1.4187 - val_accuracy: 0.4576\n",
            "Epoch 155/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9826 - accuracy: 0.5850 - val_loss: 1.3613 - val_accuracy: 0.4782\n",
            "Epoch 156/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9975 - accuracy: 0.5798 - val_loss: 1.3285 - val_accuracy: 0.4782\n",
            "Epoch 157/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9692 - accuracy: 0.5937 - val_loss: 1.2908 - val_accuracy: 0.5097\n",
            "Epoch 158/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0003 - accuracy: 0.5745 - val_loss: 1.3818 - val_accuracy: 0.4249\n",
            "Epoch 159/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9920 - accuracy: 0.5800 - val_loss: 1.3548 - val_accuracy: 0.4685\n",
            "Epoch 160/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9896 - accuracy: 0.5767 - val_loss: 1.2799 - val_accuracy: 0.5048\n",
            "Epoch 161/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0020 - accuracy: 0.5750 - val_loss: 1.3771 - val_accuracy: 0.4600\n",
            "Epoch 162/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9770 - accuracy: 0.5890 - val_loss: 1.3365 - val_accuracy: 0.4794\n",
            "Epoch 163/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0021 - accuracy: 0.5708 - val_loss: 1.3442 - val_accuracy: 0.4915\n",
            "Epoch 164/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9721 - accuracy: 0.5850 - val_loss: 1.3039 - val_accuracy: 0.5036\n",
            "Epoch 165/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9575 - accuracy: 0.5957 - val_loss: 1.4247 - val_accuracy: 0.4237\n",
            "Epoch 166/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9724 - accuracy: 0.5935 - val_loss: 1.2752 - val_accuracy: 0.5169\n",
            "Epoch 167/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9503 - accuracy: 0.6015 - val_loss: 1.4276 - val_accuracy: 0.4758\n",
            "Epoch 168/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9795 - accuracy: 0.5836 - val_loss: 1.3076 - val_accuracy: 0.4867\n",
            "Epoch 169/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9979 - accuracy: 0.5715 - val_loss: 1.4780 - val_accuracy: 0.4552\n",
            "Epoch 170/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0103 - accuracy: 0.5748 - val_loss: 1.3705 - val_accuracy: 0.4818\n",
            "Epoch 171/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9576 - accuracy: 0.5956 - val_loss: 1.3063 - val_accuracy: 0.5109\n",
            "Epoch 172/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9420 - accuracy: 0.6066 - val_loss: 1.2963 - val_accuracy: 0.5085\n",
            "Epoch 173/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9527 - accuracy: 0.5987 - val_loss: 1.2884 - val_accuracy: 0.5206\n",
            "Epoch 174/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9532 - accuracy: 0.5973 - val_loss: 1.3129 - val_accuracy: 0.4903\n",
            "Epoch 175/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9407 - accuracy: 0.6084 - val_loss: 1.3284 - val_accuracy: 0.4952\n",
            "Epoch 176/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9470 - accuracy: 0.6009 - val_loss: 1.3625 - val_accuracy: 0.4600\n",
            "Epoch 177/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9526 - accuracy: 0.5980 - val_loss: 1.5614 - val_accuracy: 0.4479\n",
            "Epoch 178/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9826 - accuracy: 0.5864 - val_loss: 1.4179 - val_accuracy: 0.4746\n",
            "Epoch 179/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9395 - accuracy: 0.6063 - val_loss: 1.3796 - val_accuracy: 0.4927\n",
            "Epoch 180/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9537 - accuracy: 0.6027 - val_loss: 1.3129 - val_accuracy: 0.5048\n",
            "Epoch 181/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9386 - accuracy: 0.6061 - val_loss: 1.3543 - val_accuracy: 0.4903\n",
            "Epoch 182/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9600 - accuracy: 0.5982 - val_loss: 1.3528 - val_accuracy: 0.4952\n",
            "Epoch 183/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9652 - accuracy: 0.5897 - val_loss: 1.3048 - val_accuracy: 0.4964\n",
            "Epoch 184/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9267 - accuracy: 0.6098 - val_loss: 1.3540 - val_accuracy: 0.4915\n",
            "Epoch 185/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9381 - accuracy: 0.6051 - val_loss: 1.3386 - val_accuracy: 0.5000\n",
            "Epoch 186/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9346 - accuracy: 0.6072 - val_loss: 1.3541 - val_accuracy: 0.5061\n",
            "Epoch 187/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9263 - accuracy: 0.6061 - val_loss: 1.3317 - val_accuracy: 0.4915\n",
            "Epoch 188/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9276 - accuracy: 0.6027 - val_loss: 1.4136 - val_accuracy: 0.4685\n",
            "Epoch 189/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9419 - accuracy: 0.5980 - val_loss: 1.4353 - val_accuracy: 0.4879\n",
            "Epoch 190/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9255 - accuracy: 0.6167 - val_loss: 1.3739 - val_accuracy: 0.4867\n",
            "Epoch 191/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9362 - accuracy: 0.6054 - val_loss: 1.3712 - val_accuracy: 0.4927\n",
            "Epoch 192/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9217 - accuracy: 0.6129 - val_loss: 1.3878 - val_accuracy: 0.4613\n",
            "Epoch 193/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9199 - accuracy: 0.6108 - val_loss: 1.4350 - val_accuracy: 0.4576\n",
            "Epoch 194/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9595 - accuracy: 0.5956 - val_loss: 1.3732 - val_accuracy: 0.5097\n",
            "Epoch 195/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9266 - accuracy: 0.6068 - val_loss: 1.3805 - val_accuracy: 0.4734\n",
            "Epoch 196/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9544 - accuracy: 0.5966 - val_loss: 1.4834 - val_accuracy: 0.4201\n",
            "Epoch 197/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9587 - accuracy: 0.5918 - val_loss: 1.4231 - val_accuracy: 0.5012\n",
            "Epoch 198/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9707 - accuracy: 0.5900 - val_loss: 1.4400 - val_accuracy: 0.4661\n",
            "Epoch 199/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.0008 - accuracy: 0.5755 - val_loss: 1.3940 - val_accuracy: 0.4818\n",
            "Epoch 200/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9482 - accuracy: 0.5987 - val_loss: 1.4031 - val_accuracy: 0.4952\n",
            "Epoch 201/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9197 - accuracy: 0.6149 - val_loss: 1.3881 - val_accuracy: 0.4964\n",
            "Epoch 202/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9296 - accuracy: 0.6082 - val_loss: 1.3564 - val_accuracy: 0.5085\n",
            "Epoch 203/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9208 - accuracy: 0.6053 - val_loss: 1.3709 - val_accuracy: 0.4976\n",
            "Epoch 204/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9322 - accuracy: 0.6053 - val_loss: 1.3634 - val_accuracy: 0.5169\n",
            "Epoch 205/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9121 - accuracy: 0.6200 - val_loss: 1.5034 - val_accuracy: 0.4395\n",
            "Epoch 206/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9673 - accuracy: 0.5893 - val_loss: 1.3627 - val_accuracy: 0.5024\n",
            "Epoch 207/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9022 - accuracy: 0.6213 - val_loss: 1.4442 - val_accuracy: 0.4564\n",
            "Epoch 208/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9123 - accuracy: 0.6162 - val_loss: 1.3724 - val_accuracy: 0.4927\n",
            "Epoch 209/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8956 - accuracy: 0.6248 - val_loss: 1.6052 - val_accuracy: 0.4201\n",
            "Epoch 210/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9170 - accuracy: 0.6189 - val_loss: 1.4590 - val_accuracy: 0.4758\n",
            "Epoch 211/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9510 - accuracy: 0.5952 - val_loss: 1.3663 - val_accuracy: 0.5061\n",
            "Epoch 212/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9544 - accuracy: 0.5945 - val_loss: 1.5165 - val_accuracy: 0.4504\n",
            "Epoch 213/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9142 - accuracy: 0.6155 - val_loss: 1.4718 - val_accuracy: 0.4625\n",
            "Epoch 214/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9143 - accuracy: 0.6149 - val_loss: 1.5013 - val_accuracy: 0.4467\n",
            "Epoch 215/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9204 - accuracy: 0.6120 - val_loss: 1.4010 - val_accuracy: 0.4939\n",
            "Epoch 216/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9207 - accuracy: 0.6117 - val_loss: 1.4214 - val_accuracy: 0.4637\n",
            "Epoch 217/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9398 - accuracy: 0.5957 - val_loss: 1.4606 - val_accuracy: 0.4673\n",
            "Epoch 218/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8984 - accuracy: 0.6205 - val_loss: 1.4561 - val_accuracy: 0.4927\n",
            "Epoch 219/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9574 - accuracy: 0.5963 - val_loss: 1.4501 - val_accuracy: 0.4867\n",
            "Epoch 220/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8976 - accuracy: 0.6222 - val_loss: 1.4248 - val_accuracy: 0.4806\n",
            "Epoch 221/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9144 - accuracy: 0.6117 - val_loss: 1.7036 - val_accuracy: 0.3668\n",
            "Epoch 222/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9570 - accuracy: 0.5963 - val_loss: 1.4385 - val_accuracy: 0.4855\n",
            "Epoch 223/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9054 - accuracy: 0.6213 - val_loss: 1.4379 - val_accuracy: 0.4903\n",
            "Epoch 224/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8807 - accuracy: 0.6309 - val_loss: 1.4564 - val_accuracy: 0.4891\n",
            "Epoch 225/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8846 - accuracy: 0.6298 - val_loss: 1.4502 - val_accuracy: 0.4709\n",
            "Epoch 226/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8952 - accuracy: 0.6241 - val_loss: 1.4306 - val_accuracy: 0.4964\n",
            "Epoch 227/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9440 - accuracy: 0.5947 - val_loss: 1.4985 - val_accuracy: 0.4734\n",
            "Epoch 228/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9243 - accuracy: 0.6054 - val_loss: 1.5204 - val_accuracy: 0.4564\n",
            "Epoch 229/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8971 - accuracy: 0.6213 - val_loss: 1.4988 - val_accuracy: 0.4600\n",
            "Epoch 230/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9056 - accuracy: 0.6151 - val_loss: 1.3994 - val_accuracy: 0.5061\n",
            "Epoch 231/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8824 - accuracy: 0.6271 - val_loss: 1.4602 - val_accuracy: 0.4952\n",
            "Epoch 232/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8913 - accuracy: 0.6239 - val_loss: 1.5095 - val_accuracy: 0.4564\n",
            "Epoch 233/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8820 - accuracy: 0.6257 - val_loss: 1.4574 - val_accuracy: 0.5061\n",
            "Epoch 234/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8862 - accuracy: 0.6255 - val_loss: 1.4598 - val_accuracy: 0.4927\n",
            "Epoch 235/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8783 - accuracy: 0.6338 - val_loss: 1.5747 - val_accuracy: 0.4443\n",
            "Epoch 236/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8931 - accuracy: 0.6234 - val_loss: 1.4572 - val_accuracy: 0.4903\n",
            "Epoch 237/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8715 - accuracy: 0.6359 - val_loss: 1.5283 - val_accuracy: 0.4528\n",
            "Epoch 238/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9005 - accuracy: 0.6276 - val_loss: 1.4758 - val_accuracy: 0.5048\n",
            "Epoch 239/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8866 - accuracy: 0.6281 - val_loss: 1.6343 - val_accuracy: 0.4237\n",
            "Epoch 240/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9054 - accuracy: 0.6210 - val_loss: 1.6216 - val_accuracy: 0.4492\n",
            "Epoch 241/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9564 - accuracy: 0.5975 - val_loss: 1.4516 - val_accuracy: 0.4988\n",
            "Epoch 242/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.9008 - accuracy: 0.6148 - val_loss: 1.4390 - val_accuracy: 0.4988\n",
            "Epoch 243/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8654 - accuracy: 0.6393 - val_loss: 1.4607 - val_accuracy: 0.5024\n",
            "Epoch 244/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8735 - accuracy: 0.6333 - val_loss: 1.4558 - val_accuracy: 0.5097\n",
            "Epoch 245/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8553 - accuracy: 0.6469 - val_loss: 1.5054 - val_accuracy: 0.4891\n",
            "Epoch 246/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8773 - accuracy: 0.6290 - val_loss: 1.5472 - val_accuracy: 0.4939\n",
            "Epoch 247/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8911 - accuracy: 0.6264 - val_loss: 1.4673 - val_accuracy: 0.5036\n",
            "Epoch 248/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8882 - accuracy: 0.6257 - val_loss: 1.5846 - val_accuracy: 0.4225\n",
            "Epoch 249/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8809 - accuracy: 0.6279 - val_loss: 1.5031 - val_accuracy: 0.4697\n",
            "Epoch 250/250\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8768 - accuracy: 0.6283 - val_loss: 1.5083 - val_accuracy: 0.5036\n",
            "26/26 - 0s - loss: 1.5083 - accuracy: 0.5036 - 70ms/epoch - 3ms/step\n",
            "Test accuracy: 0.5036319494247437\n",
            "26/26 [==============================] - 0s 1ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.88      0.65       328\n",
            "           1       0.31      0.05      0.09       153\n",
            "           2       0.47      0.38      0.42       212\n",
            "           3       0.51      0.29      0.37       106\n",
            "           4       0.77      0.37      0.50        27\n",
            "\n",
            "    accuracy                           0.50       826\n",
            "   macro avg       0.51      0.39      0.41       826\n",
            "weighted avg       0.47      0.50      0.45       826\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation"
      ],
      "metadata": {
        "id": "f-J1imdRU66S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_columns = train_labels.shape[1]\n",
        "\n",
        "for col in range(num_columns):\n",
        "    column_frequencies = np.sum(train_labels[:, col])\n",
        "\n",
        "    plt.bar([col], [column_frequencies], label=f'Column {col + 1}')\n",
        "\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Frequency of 1s\")\n",
        "plt.title(\"Frequency of 1s in Each Column\")\n",
        "plt.xticks(range(num_columns))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "num_columns = val_labels.shape[1]\n",
        "\n",
        "# Create bar graphs for val_labels\n",
        "plt.figure(figsize=(12, 4))\n",
        "for col in range(num_columns):\n",
        "    column_frequencies = np.sum(val_labels[:, col])\n",
        "\n",
        "    plt.subplot(1, 2, 1)  # Subplot for val_labels\n",
        "    plt.bar([col], [column_frequencies], label=f'Column {col + 1}')\n",
        "\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Frequency of 1s\")\n",
        "plt.title(\"Frequency of 1s in Validation Labels\")\n",
        "plt.xticks(range(num_columns))\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Create bar graphs for test_labels\n",
        "for col in range(num_columns):\n",
        "    column_frequencies = np.sum(test_labels[:, col])\n",
        "\n",
        "    plt.subplot(1, 2, 2)  # Subplot for test_labels\n",
        "    plt.bar([col], [column_frequencies], label=f'Column {col + 1}')\n",
        "\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Frequency of 1s\")\n",
        "plt.title(\"Frequency of 1s in Test Labels\")\n",
        "plt.xticks(range(num_columns))\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IVB4ZlMYU4Xj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "outputId": "dc77c039-710d-40d1-d13d-428ace110ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all labels (train, validation, and test)\n",
        "all_labels = np.vstack((train_labels, val_labels, test_labels))\n",
        "\n",
        "# Calculate the total frequency of 1s\n",
        "total_frequencies = np.sum(all_labels, axis=0)\n",
        "\n",
        "# Create a bar graph to visualize the total frequencies\n",
        "num_columns = all_labels.shape[1]\n",
        "\n",
        "plt.bar(range(num_columns), total_frequencies)\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Total Frequency of 1s\")\n",
        "plt.title(\"Total Frequency of 1s Across All Labels\")\n",
        "plt.xticks(range(num_columns))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CoocrUtGU6Rq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "b43cf81f-f822-44b1-b840-70c09b2bbd33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print frequency of 1 for each column\n",
        "for i in range(all_labels.shape[1]):\n",
        "    column_frequency = np.sum(all_labels[:, i])\n",
        "    print(f\"Frequency in column {i+1} = {column_frequency}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crRTIo744lL_",
        "outputId": "d578e222-fb42-4906-a663-52d80e81fba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency in column 1 = 6509.0\n",
            "Frequency in column 2 = 2990.0\n",
            "Frequency in column 3 = 4350.0\n",
            "Frequency in column 4 = 2182.0\n",
            "Frequency in column 5 = 502.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count occurrences for each grade\n",
        "grade_counts = np.sum(train_labels, axis=0)\n",
        "\n",
        "# Bar plot\n",
        "grades = [f\"Grade {i}\" for i in range(len(grade_counts))]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(grades, grade_counts, color=\"#b6d7a8\", label=\"Grade Counts\")\n",
        "plt.ylabel('Number of samples')\n",
        "plt.xlabel(\"Grades\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "dLqk2A81HWcQ",
        "outputId": "c3f65a4f-b35b-4745-a1a1-235565b7f461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}