{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ3yZzIm8S7Rt8iL56uQbj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NvdSuni/Thesis-code-complete/blob/main/MRnet_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQtwSBWGenjL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XSA3Xqoqerm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import __version__ as sklearn_version\n",
        "\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "print(f\"matplotlib: {matplotlib.__version__}\")\n",
        "print(f\"scikit-learn: {sklearn_version}\")\n"
      ],
      "metadata": {
        "id": "fC5Pzzb4esWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "id": "Uz76bP7tetPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Master Thesis/MRNet-v1.0/train'\n",
        "\n",
        "#Define the common dimensions\n",
        "new_slices = 17\n",
        "new_height = 256\n",
        "new_width = 256\n",
        "\n",
        "num_scans = 1130\n",
        "\n",
        "#Process and save the sagittal plane\n",
        "cropped_sagittal = np.zeros((num_scans, new_slices, new_height, new_width))\n",
        "for i in range(num_scans):\n",
        "    sagittal_filename = os.path.join(data_dir, 'sagittal', f'{i:04d}.npy')\n",
        "    sagittal_scan = np.load(sagittal_filename)\n",
        "\n",
        "    start_slice = max((sagittal_scan.shape[0] - new_slices) // 2, 0)\n",
        "\n",
        "    #Extract middle slices\n",
        "    sagittal_scan = sagittal_scan[start_slice:start_slice + new_slices, :new_height, :new_width]\n",
        "\n",
        "    cropped_sagittal[i] = sagittal_scan\n",
        "\n",
        "output_file_sagittal = '/content/drive/MyDrive/Master Thesis/MRNet-v1.0/cropped_sagittal.npy'\n",
        "np.save(output_file_sagittal, cropped_sagittal)"
      ],
      "metadata": {
        "id": "lR6h3rI2AMgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Master Thesis/MRNet-v1.0/test'\n",
        "\n",
        "#Define the common dimensions\n",
        "new_slices = 17\n",
        "new_height = 256\n",
        "new_width = 256\n",
        "\n",
        "num_scans = 120\n",
        "\n",
        "#Process and save the sagittal plane\n",
        "cropped_sagittal = np.zeros((num_scans, new_slices, new_height, new_width))\n",
        "for i in range(num_scans):\n",
        "    sagittal_filename = os.path.join(data_dir, 'sagittal', f'{i:04d}.npy')\n",
        "    sagittal_scan = np.load(sagittal_filename)\n",
        "\n",
        "    start_slice = max((sagittal_scan.shape[0] - new_slices) // 2, 0)\n",
        "\n",
        "    #Extract the middle slices\n",
        "    sagittal_scan = sagittal_scan[start_slice:start_slice + new_slices, :new_height, :new_width]\n",
        "\n",
        "    cropped_sagittal[i] = sagittal_scan\n",
        "\n",
        "output_file_sagittal = '/content/drive/MyDrive/Master Thesis/MRNet-v1.0/cropped_sagittal_test.npy'\n",
        "np.save(output_file_sagittal, cropped_sagittal)"
      ],
      "metadata": {
        "id": "SjXf1BvmAgJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MRnet_dataset = os.chdir(\"/content/drive/My Drive/Tilburg University/Master Thesis/MRNet-v1.0\")\n",
        "data_train = pd.read_csv(\"/content/drive/My Drive/Tilburg University/Master Thesis/MRNet-v1.0/train-complete.csv\", encoding='utf-8')\n",
        "data_sagittal = np.load(\"/content/drive/My Drive/Tilburg University/Master Thesis/MRNet-v1.0/cropped_sagittal.npy\")\n",
        "X_test_MRNet = np.load(\"/content/drive/My Drive/Tilburg University/Master Thesis/MRNet-v1.0/cropped_sagittal_test.npy\")"
      ],
      "metadata": {
        "id": "2HUC-4qNeyfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "Xrqx7A_9e5UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = data_train.drop([\"ID\"], axis = \"columns\")\n",
        "data_train.head()"
      ],
      "metadata": {
        "id": "TNmVtsKbe3CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(data_sagittal, data_train, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "1Rl-bRbre4IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "healthy_samples = (y_train[\"abnormal\"] == 0)\n",
        "\n",
        "y_train[\"healthy\"] = np.where(healthy_samples, 1, 0)\n",
        "\n",
        "healthy_samples_val = (y_val[\"abnormal\"] == 0)\n",
        "y_val[\"healthy\"] = np.where(healthy_samples_val, 1, 0)"
      ],
      "metadata": {
        "id": "CYHhPCXne9hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[\"label\"] = y_train.apply(lambda row: [row[\"abnormal\"], row[\"acl\"], row[\"meniscus\"], row[\"healthy\"]], axis=1)\n",
        "y_val[\"label\"] = y_val.apply(lambda row: [row[\"abnormal\"], row[\"acl\"], row[\"meniscus\"], row[\"healthy\"]], axis=1)\n",
        "\n",
        "y_train = np.array(y_train[\"label\"].tolist())\n",
        "y_val = np.array(y_val[\"label\"].tolist())\n",
        "\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "CGobcO52fAER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_first_column = y_train[:, 0]\n",
        "y_val_first_column = y_val[:, 0]"
      ],
      "metadata": {
        "id": "suIdo41ZfK1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/y_train_MRNet.npy\", y_train_first_column)\n",
        "np.save(\"/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/y_val_MRNet.npy\", y_val_first_column)"
      ],
      "metadata": {
        "id": "w9Ht8kSxfNg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalisation\n",
        "X_train_normalized = X_train / 255.0\n",
        "X_val_normalized = X_val / 255.0\n",
        "X_test_MRNet_normalised = X_test_MRNet / 255.0"
      ],
      "metadata": {
        "id": "gcr12GFzfOjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardisation\n",
        "mean = np.mean(X_train_normalized, axis=(0, 1, 2, 3))\n",
        "std = np.std(X_train_normalized, axis=(0, 1, 2, 3))\n",
        "\n",
        "#Standardize the data for training and validation sets\n",
        "X_train_standardized = (X_train_normalized - mean) / std\n",
        "X_val_standardized = (X_val_normalized - mean) / std\n",
        "X_test_MRNet_normalised_standardised = (X_test_MRNet_normalised - mean) / std\n"
      ],
      "metadata": {
        "id": "SidGi3AUfPYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/drive/My Drive/Tilburg University/Master Thesis/MRNet-v1.0/X_train_standardized_MRNet\", X_train_standardized)\n",
        "np.save(\"/content/drive/My Drive/Tilburg University/Master Thesis/MRNet-v1.0/X_val_standardized_MRNet\",X_val_standardized)"
      ],
      "metadata": {
        "id": "CD0koiecfaCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_standardized = np.load(\"/content/drive/My Drive/Tilburg University/Master Thesis/MRNet-v1.0/X_train_standardized_MRNet.npy\")\n",
        "X_val_standardized = np.load(\"/content/drive/My Drive/Tilburg University/Master Thesis/MRNet-v1.0/X_val_standardized_MRNet.npy\")\n",
        "y_train = np.load(\"/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/y_train_MRNet.npy\")\n",
        "y_val = np.load(\"/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/y_val_MRNet.npy\")\n",
        "X_test_MRNet = np.load(\"/content/drive/My Drive/Tilburg University/Master Thesis/MRNet-v1.0/X_test_MRNet.npy\")\n",
        "y_test_MRNet = np.load(\"/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/y_test_MRNet.npy\")"
      ],
      "metadata": {
        "id": "AaC11OLOfbPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FEATURE EXTRACTION\n"
      ],
      "metadata": {
        "id": "jRWO82vtfd0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "#Convolutional layers\n",
        "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(17, 256, 256, 1), name=\"COV1\"))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu', name=\"COV2\"))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "model.add(Flatten(name=\"flatten\"))\n",
        "\n",
        "model.add(Dense(128, activation='relu', name=\"dense_layer_1\"))\n",
        "model.add(Dense(1, activation='sigmoid', name=\"output_layer\"))  # Use 1 unit with sigmoid for binary classification\n",
        "\n",
        "#Define a custom learning rate\n",
        "custom_learning_rate = 0.00000001\n",
        "\n",
        "optimizer = Adam(learning_rate=custom_learning_rate)\n",
        "\n",
        "#Compile the model\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_standardized, y_train, batch_size=8, epochs=20, validation_data=(X_val_standardized, y_val))\n"
      ],
      "metadata": {
        "id": "nQlzMSXPfeJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract features from the CNN model\n",
        "cnn_feature_extractor_MRI = Model(inputs=model.input, outputs=model.get_layer('COV2').output)\n",
        "\n",
        "cnn_feature_extractor_MRI.save('cnn_feature_extractor_model_MRI')\n",
        "\n"
      ],
      "metadata": {
        "id": "lQdzXxDsfn-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_feature_extractor_MRI = load_model('cnn_feature_extractor_model_MRI')"
      ],
      "metadata": {
        "id": "UMpM4yM7fsdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "cnn_features_X_train_MRNet = []\n",
        "\n",
        "for i in range(0, len(X_train_standardized), batch_size):\n",
        "    batch_data = X_train_standardized[i:i + batch_size]\n",
        "    features = cnn_feature_extractor_MRI.predict(batch_data)\n",
        "    cnn_features_X_train_MRNet.append(features)\n",
        "\n",
        "#Concatenate the results\n",
        "cnn_features_X_train_MRNet = np.concatenate(cnn_features_X_train_MRNet, axis=0)\n",
        "\n",
        "np.save(\"/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/cnn_features_X_train_MRNet2.npy\", cnn_features_X_train_MRNet)"
      ],
      "metadata": {
        "id": "5ejpXkVofupU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_features_X_val_MRNet = []\n",
        "\n",
        "for i in range(0, len(X_val_standardized), batch_size):\n",
        "    batch_data = X_val_standardized[i:i + batch_size]\n",
        "    features = cnn_feature_extractor_MRI.predict(batch_data)\n",
        "    cnn_features_X_val_MRNet.append(features)\n",
        "\n",
        "#Concatenate the results\n",
        "cnn_features_X_val_MRNet = np.concatenate(cnn_features_X_val_MRNet, axis=0)\n",
        "\n",
        "np.save(\"/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/cnn_features_X_val_MRNet2.npy\", cnn_features_X_val_MRNet)"
      ],
      "metadata": {
        "id": "Ndsk2O7Afzsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_features_X_test_MRNet = []\n",
        "\n",
        "for i in range(0, len(X_test_MRNet_normalised_standardised), batch_size):\n",
        "    batch_data = X_test_MRNet_normalised_standardised[i:i + batch_size]\n",
        "    features = cnn_feature_extractor_MRI.predict(batch_data)\n",
        "    cnn_features_X_test_MRNet.append(features)\n",
        "\n",
        "#Concatenate the results\n",
        "cnn_features_X_test_MRNet = np.concatenate(cnn_features_X_test_MRNet, axis=0)\n",
        "\n",
        "np.save(\"/content/drive/My Drive/Tilburg University/Master Thesis/Combined data/cnn_features_X_test_MRNet.npy\", cnn_features_X_test_MRNet)"
      ],
      "metadata": {
        "id": "sxswgUBmgaEh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}